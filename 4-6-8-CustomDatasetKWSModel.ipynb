{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO4-CY_TCZZS"
      },
      "source": [
        "# Training Your Custom Dataset Keyword Spotting Model\n",
        "\n",
        "It is now time for you to train your own custom keyword spotting model using your Custom Dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0HQ7WkLUxD4"
      },
      "source": [
        "#Setup\n",
        "\n",
        "### Import packages\n",
        "Clone the TensorFlow Github Repository, which contains the relevant code required to run this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "olCcGuF7GRVO",
        "outputId": "8f802f0b-0a4c-4023-f0a4-7891db30f26f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-14 08:44:11--  https://github.com/tensorflow/tensorflow/archive/v2.4.1.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/tensorflow/tensorflow/zip/refs/tags/v2.4.1 [following]\n",
            "--2025-12-14 08:44:11--  https://codeload.github.com/tensorflow/tensorflow/zip/refs/tags/v2.4.1\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.112.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.112.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘v2.4.1.zip’\n",
            "\n",
            "v2.4.1.zip              [     <=>            ]  66.13M  12.7MB/s    in 5.7s    \n",
            "\n",
            "2025-12-14 08:44:17 (11.5 MB/s) - ‘v2.4.1.zip’ saved [69346072]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/tensorflow/tensorflow/archive/v2.4.1.zip\n",
        "!unzip v2.4.1.zip &> 0\n",
        "!mv tensorflow-2.4.1/ tensorflow/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2Xne8hbyzT1"
      },
      "source": [
        "### Important - Reverting to TF1 - Start\n",
        "We need to install Tensorflow V1 in order to use the required features for Keyword Spotting.\n",
        "\n",
        "**Run the following cell and click \"restart runtime\" that appears once it completes.**\n",
        "\n",
        "**Do not rerun the previous cells after restarting the runtime.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G43nVAIdy3uD",
        "outputId": "ee2ae219-fb78-4cd4-b4f2-51b75cc718d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.15.0 (from versions: 2.12.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-gpu==1.15.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow-gpu==1.15.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAPKHaKmy5RN"
      },
      "source": [
        "### Important - Reverting to TF1 - End\n",
        "\n",
        "**Make sure to click the \"RESTART RUNTIME\" button in the output of the previous cell. Then move onto the next cell!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mWIJQmuTy9HU",
        "outputId": "b42ee54a-7528-4944-ff77-f3a9830b7bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.compat'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2999216879.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# assert(tf.VERSION == \"1.15.0\") #Make sure you install Tensorflow 1.15.0 and restart the runtime or this will fail!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Now that the runtime is set import things!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# We add this path so we can import the speech processing modules.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.compat'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "# assert(tf.VERSION == \"1.15.0\") #Make sure you install Tensorflow 1.15.0 and restart the runtime or this will fail!\n",
        "# Now that the runtime is set import things!\n",
        "import sys\n",
        "# We add this path so we can import the speech processing modules.\n",
        "sys.path.append(\"/content/tensorflow/tensorflow/examples/speech_commands/\")\n",
        "import input_data\n",
        "import models\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import subprocess\n",
        "import tempfile\n",
        "from google.colab import files\n",
        "!pip install ffmpeg-python &> 0\n",
        "!apt-get update && apt-get -qq install xxd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QpT5x5sL0RA"
      },
      "source": [
        "### Загрузка данных для предобучения\n",
        "Подгрузим Pete's dataset, чтобы использовать в качестве данных для предобучения. Это будет стадия пре-трейна, которая позволит предварительно обучить модель для задачи KWS, а затем мы произведем fine-tuning на данных со словами 'не слышу', 'не слышно'. Потенциально, это позволит добиться более высокого качества.\n",
        "\n",
        "Набор слов, которые детектируются в Pete's dataset: \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", “backward”, “forward”, “follow”, “learn”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE7SthPtL1lY"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
        "DATASET_DIR =  'dataset/'\n",
        "!mkdir dataset\n",
        "!tar -xf speech_commands_v0.02.tar.gz -C 'dataset'\n",
        "!rm -r -f speech_commands_v0.02.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим данные для задания"
      ],
      "metadata": {
        "id": "CNSReFJAkxeK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppOPH3NfS5W4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "file_path = '/content/drive/MyDrive/SpeechTechnologies/train_data.tar'\n",
        "with tarfile.open(file_path, 'r') as tar_train:\n",
        "    tar_train.extractall()\n"
      ],
      "metadata": {
        "id": "ikEeUxArqaXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/SpeechTechnologies/test_data.tar'\n",
        "with tarfile.open(file_path, 'r') as tar_test:\n",
        "    tar_test.extractall()"
      ],
      "metadata": {
        "id": "neUXJ_Qvq77Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_train_path = '/content/train_opus/audio'\n",
        "json_train_path = '/content/train_opus/word_bounds.json'\n",
        "base_test_path = '/test_opus/audio'"
      ],
      "metadata": {
        "id": "Q2htoXXtyhHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!apt-get install -y ffmpeg"
      ],
      "metadata": {
        "id": "voUXJkSFA0EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get install -y ffmpeg"
      ],
      "metadata": {
        "id": "R0zrmmlnCTlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "import random\n",
        "\n",
        "TARGET_SR = 16000  # Стандарт для KWS\n",
        "SEGMENT_DURATION = 2.0\n",
        "SEGMENT_SAMPLES = int(SEGMENT_DURATION * TARGET_SR)\n",
        "CONTEXT_RATIO = 0.3  # 30% контекста вокруг слова\n",
        "\n",
        "\n",
        "def load_and_convert_opus_files(base_path, json_path, sample_files=None):\n",
        "    \"\"\"\n",
        "    Загружает JSON и конвертирует OPUS файлы в аудио тензоры\n",
        "\n",
        "    Args:\n",
        "        base_path: путь к папке с OPUS файлами\n",
        "        json_path: путь к JSON файлу с аннотациями\n",
        "        sample_files: если указано, обрабатывает только эти файлы (для теста)\n",
        "\n",
        "    Returns:\n",
        "        dict: {file_id: {'audio': np.array, 'bounds': [[start1, end1], ...]}}\n",
        "    \"\"\"\n",
        "\n",
        "    # Загружаем JSON\n",
        "    with open(json_path, 'r') as f:\n",
        "        word_bounds = json.load(f)\n",
        "    print(len(word_bounds))\n",
        "\n",
        "    # Cписок OPUS файлов\n",
        "    opus_files = [f for f in os.listdir(base_path) if f.endswith('.opus')]\n",
        "    if sample_files:\n",
        "        opus_files = opus_files[:sample_files]\n",
        "    print(f\"Найдено {len(opus_files)} OPUS файлов\")\n",
        "\n",
        "    # Конвертируем каждый файл\n",
        "    audio_data = {}\n",
        "\n",
        "    for opus_file in tqdm(opus_files, desc=\"Конвертация OPUS -> WAV тензор\"):\n",
        "        if opus_file.startswith('._'):\n",
        "            continue\n",
        "        try:\n",
        "\n",
        "            file_id = opus_file.replace('.opus', '')\n",
        "            opus_path = os.path.join(base_path, opus_file)\n",
        "\n",
        "            audio_array = librosa.load(opus_path, sr=TARGET_SR, mono=True)[0]\n",
        "\n",
        "            bounds_list = word_bounds.get(file_id, [])\n",
        "\n",
        "            audio_data[file_id] = {\n",
        "                'audio': audio_array,\n",
        "                'bounds': bounds_list,\n",
        "                'duration': len(audio_array) / TARGET_SR,\n",
        "                'has_word': len(bounds_list) > 0\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при обработке {opus_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"✅ Успешно обработано {len(audio_data)} файлов\")\n",
        "    print(f\"   - Файлов с целевым словом: {sum(1 for d in audio_data.values() if d['has_word'])}\")\n",
        "    print(f\"   - Файлов без целевого слова: {sum(1 for d in audio_data.values() if not d['has_word'])}\")\n",
        "\n",
        "    return audio_data"
      ],
      "metadata": {
        "id": "hleQwNfZ58PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_train = load_and_convert_opus_files(base_train_path, json_train_path)"
      ],
      "metadata": {
        "id": "6YY-LocI-5O2",
        "outputId": "560fcf43-13ab-497b-b8d3-18c67a55a831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_and_convert_opus_files' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4291917843.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_convert_opus_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_train_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_train_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'load_and_convert_opus_files' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU8wwiKmTU8m"
      },
      "source": [
        "Then we can convert them into correctly trimmed WAV files and then store them in the appropriate folders in the ```DATASET_DIR```.\n",
        "We will use Pete's extract_loudest_section tool which you can find more documentation about here: https://github.com/petewarden/extract_loudest_section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PvD-afuDTLGS",
        "outputId": "31e7445e-2cb7-471f-f9c1-b1dff45c7539",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "find: ‘*.ogg’: No such file or directory\n",
            "basename: missing operand\n",
            "Try 'basename --help' for more information.\n",
            "Cloning into 'extract_loudest_section'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Total 58 (delta 0), reused 0 (delta 0), pack-reused 58 (from 1)\u001b[K\n",
            "Receiving objects: 100% (58/58), 49.00 KiB | 111.00 KiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n",
            "make: Entering directory '/content/extract_loudest_section'\n",
            "gcc --std=c++11 -O3 -DNDEBUG -I. -c main.cc -o /tmp/extract_loudest_section//gen/obj/./main.o\n",
            "gcc --std=c++11 -O3 -DNDEBUG -I. -c status.cc -o /tmp/extract_loudest_section//gen/obj/./status.o\n",
            "gcc --std=c++11 -O3 -DNDEBUG -I. -c wav_io.cc -o /tmp/extract_loudest_section//gen/obj/./wav_io.o\n",
            "gcc  -I. \\\n",
            "-o /tmp/extract_loudest_section//gen/bin//extract_loudest_section /tmp/extract_loudest_section//gen/obj/./main.o /tmp/extract_loudest_section//gen/obj/./status.o /tmp/extract_loudest_section//gen/obj/./wav_io.o \\\n",
            " -lstdc++ -lm\n",
            "make: Leaving directory '/content/extract_loudest_section'\n"
          ]
        }
      ],
      "source": [
        "# convert the ogg files to wavs\n",
        "!mkdir wavs\n",
        "!find *.ogg -print0 | xargs -0 basename -s .ogg | xargs -I {} ffmpeg -i {}.ogg -ar 16000 wavs/{}.wav\n",
        "!rm -r -f *.ogg\n",
        "\n",
        "# then use pete's tool to only extract 1 second clips from them for use with the KWS pipeline\n",
        "!mkdir trimmed_wavs\n",
        "!git clone https://github.com/petewarden/extract_loudest_section.git\n",
        "!make -C extract_loudest_section/\n",
        "!/tmp/extract_loudest_section/gen/bin/extract_loudest_section 'wavs/*.wav' trimmed_wavs/\n",
        "!rm -r -f /wavs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "99ydxbONTgW-",
        "outputId": "1d465308-d8b1-49b7-89e2-bf9cc6f43afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:10: SyntaxWarning: invalid escape sequence '\\.'\n",
            "<>:10: SyntaxWarning: invalid escape sequence '\\.'\n",
            "/tmp/ipython-input-742518429.py:10: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  matches = re.search('([^/_]+)_([^/_]+)\\.wav', wav_path)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-742518429.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Store them in the appropriate folders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trimmed_wavs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msearch_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwav_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "# Store them in the appropriate folders\n",
        "data_index = {}\n",
        "os.chdir('trimmed_wavs')\n",
        "search_path = os.path.join('*.wav')\n",
        "for wav_path in glob.glob(search_path):\n",
        "    original_wav_path = wav_path\n",
        "    parts = wav_path.split('_')\n",
        "    if len(parts) > 2:\n",
        "        wav_path = parts[0] + '_' + ''.join(parts[1:])\n",
        "    matches = re.search('([^/_]+)_([^/_]+)\\.wav', wav_path)\n",
        "    if not matches:\n",
        "        raise Exception('File name not in a recognized form:\"%s\"' % wav_path)\n",
        "    word = matches.group(1).lower()\n",
        "    instance = matches.group(2).lower()\n",
        "    if not word in data_index:\n",
        "      data_index[word] = {}\n",
        "    if instance in data_index[word]:\n",
        "        raise Exception('Audio instance already seen:\"%s\"' % wav_path)\n",
        "    data_index[word][instance] = original_wav_path\n",
        "\n",
        "output_dir = os.path.join('..', 'dataset')\n",
        "try:\n",
        "    os.mkdir(output_dir)\n",
        "except:\n",
        "    pass\n",
        "for word in data_index:\n",
        "  word_dir = os.path.join(output_dir, word)\n",
        "  try:\n",
        "      os.mkdir(word_dir)\n",
        "      print('Created dir: ' + word_dir)\n",
        "  except:\n",
        "      print('Storing in existing dir: ' + word_dir)\n",
        "  for instance in data_index[word]:\n",
        "    wav_path = data_index[word][instance]\n",
        "    output_path = os.path.join(word_dir, instance + '.wav')\n",
        "    shutil.copyfile(wav_path, output_path)\n",
        "os.chdir('..')\n",
        "!rm -r -f trimmed_wavs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koCYgUz6U8SW"
      },
      "source": [
        "If you would like to download your full custom dataset as a zipfile for use later you can run the commented out cell below. And then download it from the Files area by clicking on the three dots next to the zip file. Note: this command will take a little while to run as the combination of your data and Pete's dataset will be relatively large!\n",
        "\n",
        "If you would like to create a zip of just your dataset please re-run this colab (or factory reset it to wipe the memory) and skip the line that downloads Pete's dataset. That said, we STRONGLY suggest you do use Pete's dataset for any actual training you do!\n",
        "\n",
        "Finally if you have room in your Google Drive and would like to load and store your dataset from there in the future you can mount your Google Drive in Colab [as described in this blog post](https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg_AHa7VU7wG"
      },
      "outputs": [],
      "source": [
        "#!zip -r myKWSDataset.zip dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaVtYN4nlCft"
      },
      "source": [
        "### Configure Your Model!\n",
        "Ok now that your custom dataset is all ready to go we'll need to select your keywords and model settings with which to train!\n",
        "\n",
        "```WANTED_WORDS``` = A comma-delimited string of the words you want to train for (e.g., \"yes,no\").\n",
        "\n",
        "**Make sure to input the keywords you collected!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ludfxbNIaegy"
      },
      "outputs": [],
      "source": [
        "WANTED_WORDS = \"yes,no\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8VBOW8Ob7ys"
      },
      "source": [
        "The number of training steps and learning rates can be specified as comma-separated strings to define the amount/rate at each stage. For example, ```TRAINING_STEPS=\"12000,3000\"``` and ```LEARNING_RATE=\"0.001,0.0001\"``` will run 12,000 training steps with a rate of 0.001 followed by 3,000 final steps with a learning rate of 0.0001. These are good default values to work off of when you choose your values as the course staff has gotten this to work well with those values in the past!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1eWOvdfcPlo"
      },
      "outputs": [],
      "source": [
        "TRAINING_STEPS = \"6000,1000\"\n",
        "LEARNING_RATE = \"0.001,0.0001\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q36xx9vcQVp"
      },
      "source": [
        "We suggest you leave the ```MODEL_ARCHITECTURE``` as tiny_conv the first time but if you would like to do this again and explore additional models some options are: ```single_fc, conv, low_latency_conv, low_latency_svdf, tiny_embedding_conv```. **Do remember if you switch the model type you may need to update the C++ code to include the ```tflite::AllOpsResolver``` to make sure you have all of the neccessary ops!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3Y1phllccMn"
      },
      "outputs": [],
      "source": [
        "MODEL_ARCHITECTURE = 'tiny_conv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhj3jQr0cfCt"
      },
      "outputs": [],
      "source": [
        "# Calculate the total number of steps, which is used to identify the checkpoint\n",
        "# file name.\n",
        "TOTAL_STEPS = str(sum(map(lambda string: int(string), TRAINING_STEPS.split(\",\"))))\n",
        "\n",
        "# Print the configuration to confirm it\n",
        "print(\"Training these words: %s\" % WANTED_WORDS)\n",
        "print(\"Training steps in each stage: %s\" % TRAINING_STEPS)\n",
        "print(\"Learning rate in each stage: %s\" % LEARNING_RATE)\n",
        "print(\"Total number of training steps: %s\" % TOTAL_STEPS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCgeOpvY9pAi"
      },
      "source": [
        "**We suggest that you do not modify** the following constants as they include filepaths used in this notebook and data that is shared during training and inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nd1iM1o2ymvA"
      },
      "outputs": [],
      "source": [
        "# Calculate the percentage of 'silence' and 'unknown' training samples required\n",
        "# to ensure that we have equal number of samples for each label.\n",
        "number_of_labels = WANTED_WORDS.count(',') + 1\n",
        "number_of_total_labels = number_of_labels + 2 # for 'silence' and 'unknown' label\n",
        "equal_percentage_of_training_samples = int(100.0/(number_of_total_labels))\n",
        "SILENT_PERCENTAGE = equal_percentage_of_training_samples\n",
        "UNKNOWN_PERCENTAGE = equal_percentage_of_training_samples\n",
        "\n",
        "# Constants used during training only\n",
        "VERBOSITY = 'DEBUG'\n",
        "EVAL_STEP_INTERVAL = '1000'\n",
        "SAVE_STEP_INTERVAL = '1000'\n",
        "\n",
        "# Constants for training directories and filepaths\n",
        "LOGS_DIR = 'logs/'\n",
        "TRAIN_DIR = 'train/' # for training checkpoints and other files.\n",
        "\n",
        "# Constants for inference directories and filepaths\n",
        "import os\n",
        "MODELS_DIR = 'models'\n",
        "if not os.path.exists(MODELS_DIR):\n",
        "  os.mkdir(MODELS_DIR)\n",
        "MODEL_TF = os.path.join(MODELS_DIR, 'KWS_custom.pb')\n",
        "MODEL_TFLITE = os.path.join(MODELS_DIR, 'KWS_custom.tflite')\n",
        "FLOAT_MODEL_TFLITE = os.path.join(MODELS_DIR, 'KWS_custom_float.tflite')\n",
        "MODEL_TFLITE_MICRO = os.path.join(MODELS_DIR, 'KWS_custom.cc')\n",
        "SAVED_MODEL = os.path.join(MODELS_DIR, 'KWS_custom_saved_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiKisyFN4R6W"
      },
      "source": [
        "**Be careful if you modify** the following constants as they will have downstream effects on the C++ code which you will then have to change. This mainly relate to hyperparaemeters for quantization and preprocessing. The first time you train a custom model **we suggest you do not modify these as well.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbdtOiLV4SMD"
      },
      "outputs": [],
      "source": [
        "# Constants which are shared during training and inference\n",
        "PREPROCESS = 'micro'\n",
        "WINDOW_STRIDE = 20\n",
        "\n",
        "# Constants for Quantization\n",
        "QUANT_INPUT_MIN = 0.0\n",
        "QUANT_INPUT_MAX = 26.0\n",
        "QUANT_INPUT_RANGE = QUANT_INPUT_MAX - QUANT_INPUT_MIN\n",
        "\n",
        "# Constants for audio process during Quantization and Evaluation\n",
        "SAMPLE_RATE = 16000\n",
        "CLIP_DURATION_MS = 1000\n",
        "WINDOW_SIZE_MS = 30.0\n",
        "FEATURE_BIN_COUNT = 40\n",
        "BACKGROUND_FREQUENCY = 0.8\n",
        "BACKGROUND_VOLUME_RANGE = 0.1\n",
        "TIME_SHIFT_MS = 100.0\n",
        "\n",
        "# Use the custom local dataset and set the tes/val/train split\n",
        "DATA_URL = ''\n",
        "VALIDATION_PERCENTAGE = 10\n",
        "TESTING_PERCENTAGE = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UczQKtqLi7OJ"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "### Load in TensorBoard to visulaize the training process.\n",
        "\n",
        "As training progresses you should see the training status show up in the Tensorboard area. If this works it is very helpful for analyzing your training progress. Unfortunately, the staff has found that it sometimes doesn't start showing data for a while (~15 minutes) and sometimes doesn't show data until training completes (and instead shows ```No dashboards are active for the current data set```.). If it is working and then stops updating look to the top of the cell and click reconnect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozkJqgSkU_qI"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {LOGS_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gweRC9eWVdg2"
      },
      "source": [
        "### Launch Training\n",
        "\n",
        "**ARE YOU USING A GPU INSTANCE!?!?!**\n",
        "Make sure to check ```runtime->change runtime type``` as a GPU runtime will take ~2 hours to complete training whereas a CPU runtime with take ~10 hours!\n",
        "\n",
        "**Importantly, every hour or so you should make sure to remind colab you are still there by clicking around in a cell or clicking reconnect on the tensorboard cell (after 90 minutes of no activity Colab may kill the instance)!** But/and beyond that you can absolutely minimize the window and continue with other work.\n",
        "\n",
        "**If you run training a second time** we suggest restarting your runtime ```runtime->restart runtime``` in order to ensure all caches are cleared. However, do note that doing so will clear all cells and so you will have to run the entire file again.\n",
        "\n",
        "If you would like to get more information on the training script you can find the source code for the script [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/train.py). In short it sets up the optimizer and preprocessor based on all of the flags we pass in!\n",
        "\n",
        "Finally, by setting the ```VERBOSITY = 'DEBUG'``` above be aware that the training cell will print A LOT of information. Specifically you will get the accuracy and loss at each step as well as a confusion matrix every 1000 steps. We hope that is helpful in case TensorBoard fails to work. If you would like to run with less printouts you can change the setting to ```WARN``` or ```FATAL```. You will find this in the \"Configure Your Model!\" section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZw3VNlnla-J"
      },
      "outputs": [],
      "source": [
        "!python tensorflow/tensorflow/examples/speech_commands/train.py \\\n",
        "  --data_dir={DATASET_DIR} \\\n",
        "  --data_url={DATA_URL} \\\n",
        "  --wanted_words={WANTED_WORDS} \\\n",
        "  --silence_percentage={SILENT_PERCENTAGE} \\\n",
        "  --unknown_percentage={UNKNOWN_PERCENTAGE} \\\n",
        "  --preprocess={PREPROCESS} \\\n",
        "  --window_stride={WINDOW_STRIDE} \\\n",
        "  --model_architecture={MODEL_ARCHITECTURE} \\\n",
        "  --how_many_training_steps={TRAINING_STEPS} \\\n",
        "  --learning_rate={LEARNING_RATE} \\\n",
        "  --train_dir={TRAIN_DIR} \\\n",
        "  --summaries_dir={LOGS_DIR} \\\n",
        "  --verbosity={VERBOSITY} \\\n",
        "  --eval_step_interval={EVAL_STEP_INTERVAL} \\\n",
        "  --save_step_interval={SAVE_STEP_INTERVAL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjBn-NapWsho"
      },
      "source": [
        "# Generating your Model\n",
        "Just like with the pre-trained model we will now take the final checkpoint and convert it into a quantized TensorFlow Lite model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQUJLrdS-ftl"
      },
      "source": [
        "### Generate a TensorFlow Model for Inference\n",
        "\n",
        "Combine relevant training results (graph, weights, etc) into a single file for inference. This process is known as freezing a model and the resulting model is known as a frozen model/graph, as it cannot be further re-trained after this process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyc3_eLh9sAg"
      },
      "outputs": [],
      "source": [
        "!rm -rf {SAVED_MODEL}\n",
        "!python tensorflow/tensorflow/examples/speech_commands/freeze.py \\\n",
        "--wanted_words=$WANTED_WORDS \\\n",
        "--window_stride_ms=$WINDOW_STRIDE \\\n",
        "--preprocess=$PREPROCESS \\\n",
        "--model_architecture=$MODEL_ARCHITECTURE \\\n",
        "--start_checkpoint=$TRAIN_DIR$MODEL_ARCHITECTURE'.ckpt-'{TOTAL_STEPS} \\\n",
        "--save_format=saved_model \\\n",
        "--output_file={SAVED_MODEL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DBGDxVI-nKG"
      },
      "source": [
        "### Generate a TensorFlow Lite Model\n",
        "\n",
        "Convert the frozen graph into a TensorFlow Lite model, which is fully quantized for use with embedded devices. The following cell will also print the model size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNQdAplJV1fz"
      },
      "outputs": [],
      "source": [
        "model_settings = models.prepare_model_settings(\n",
        "    len(input_data.prepare_words_list(WANTED_WORDS.split(','))),\n",
        "    SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
        "    WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
        "audio_processor = input_data.AudioProcessor(\n",
        "    DATA_URL, DATASET_DIR,\n",
        "    SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE,\n",
        "    WANTED_WORDS.split(','), VALIDATION_PERCENTAGE,\n",
        "    TESTING_PERCENTAGE, model_settings, LOGS_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-rvCCM2G58N"
      },
      "source": [
        "**Note: if the below cell fails it might be because you do not have enough data to have 100 recordings in the representative dataset!** If this happens you will see an error that says something like ```ValueError: cannot reshape array of size 0 into shape (1,1960)```. To help you fix this we have added a ```print(i)``` into the loop. As such, all you have to do is change the ```REP_DATA_SIZE``` variable to be equal to the last integer value printed out by the loop and then re-run the cell!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBj_AyCh1cC0"
      },
      "outputs": [],
      "source": [
        "REP_DATA_SIZE = 100\n",
        "with tf.Session() as sess:\n",
        "  # with tf.compat.v1.Session() as sess: #replaces the above line for use with TF2.x\n",
        "  float_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
        "  float_tflite_model = float_converter.convert()\n",
        "  float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\n",
        "  print(\"Float model is %d bytes\" % float_tflite_model_size)\n",
        "\n",
        "  converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  converter.inference_input_type = tf.lite.constants.INT8\n",
        "  # converter.inference_input_type = tf.compat.v1.lite.constants.INT8 #replaces the above line for use with TF2.x\n",
        "  converter.inference_output_type = tf.lite.constants.INT8\n",
        "  # converter.inference_output_type = tf.compat.v1.lite.constants.INT8 #replaces the above line for use with TF2.x\n",
        "  def representative_dataset_gen():\n",
        "    for i in range(REP_DATA_SIZE):\n",
        "      data, _ = audio_processor.get_data(1, i*1, model_settings,\n",
        "                                         BACKGROUND_FREQUENCY,\n",
        "                                         BACKGROUND_VOLUME_RANGE,\n",
        "                                         TIME_SHIFT_MS,\n",
        "                                         'testing',\n",
        "                                         sess)\n",
        "      flattened_data = np.array(data.flatten(), dtype=np.float32).reshape(1, 1960)\n",
        "      print(i)\n",
        "      yield [flattened_data]\n",
        "  converter.representative_dataset = representative_dataset_gen\n",
        "  tflite_model = converter.convert()\n",
        "  tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\n",
        "  print(\"Quantized model is %d bytes\" % tflite_model_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeLiDZTbLkzv"
      },
      "source": [
        "### Testing the accuracy after Quantization\n",
        "\n",
        "Verify that the model we've exported is still accurate, using the TF Lite Python API and our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQsEteKRLryJ"
      },
      "outputs": [],
      "source": [
        "# Helper function to run inference\n",
        "def run_tflite_inference_testSet(tflite_model_path, model_type=\"Float\"):\n",
        "  #\n",
        "  # Load test data\n",
        "  #\n",
        "  np.random.seed(0) # set random seed for reproducible test results.\n",
        "  with tf.Session() as sess:\n",
        "    # with tf.compat.v1.Session() as sess: #replaces the above line for use with TF2.x\n",
        "    test_data, test_labels = audio_processor.get_data(\n",
        "        -1, 0, model_settings, BACKGROUND_FREQUENCY, BACKGROUND_VOLUME_RANGE,\n",
        "        TIME_SHIFT_MS, 'testing', sess)\n",
        "  test_data = np.expand_dims(test_data, axis=1).astype(np.float32)\n",
        "\n",
        "  #\n",
        "  # Initialize the interpreter\n",
        "  #\n",
        "  interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "  interpreter.allocate_tensors()\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  #\n",
        "  # For quantized models, manually quantize the input data from float to integer\n",
        "  #\n",
        "  if model_type == \"Quantized\":\n",
        "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "    test_data = test_data / input_scale + input_zero_point\n",
        "    test_data = test_data.astype(input_details[\"dtype\"])\n",
        "\n",
        "  #\n",
        "  # Evaluate the predictions\n",
        "  #\n",
        "  correct_predictions = 0\n",
        "  for i in range(len(test_data)):\n",
        "    interpreter.set_tensor(input_details[\"index\"], test_data[i])\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "    top_prediction = output.argmax()\n",
        "    correct_predictions += (top_prediction == test_labels[i])\n",
        "\n",
        "  print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n",
        "      model_type, (correct_predictions * 100) / len(test_data), len(test_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-pD52Na6jRa"
      },
      "outputs": [],
      "source": [
        "# Compute float model accuracy\n",
        "run_tflite_inference_testSet(FLOAT_MODEL_TFLITE)\n",
        "\n",
        "# Compute quantized model accuracy\n",
        "run_tflite_inference_testSet(MODEL_TFLITE, model_type='Quantized')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf6CQuooWQy5"
      },
      "source": [
        "### Generate a TensorFlow Lite for Microcontrollers Model\n",
        "To convert the TensorFlow Lite quantized model into a C source file that can be loaded by TensorFlow Lite for Microcontrollers on Arduino we simply need to use the ```xxd``` tool to convert the ```.tflite``` file into a ```.cc``` file (just like we did in the pervious section)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwStW8XCWXXX"
      },
      "outputs": [],
      "source": [
        "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
        "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
        "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-AhT48A17Eo"
      },
      "source": [
        "That's it! You've successfully converted your TensorFlow Lite model into a TensorFlow Lite for Microcontrollers model! Run the cell below to print out its contents which we'll need for our next step, deploying the model using the Arudino IDE!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79XIezycHhp6"
      },
      "outputs": [],
      "source": [
        "!cat {MODEL_TFLITE_MICRO}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQxu8s15HbfJ"
      },
      "source": [
        "To download your model for use at a later date:\n",
        "\n",
        "1. On the left of the UI click on the folder icon\n",
        "2. Click on the three dots to the right of the ```.cc``` file you just generated and select \"download.\" The file can be found at ```models/{MODEL_TFLITE_MICRO}``` which by default is ```models/KWS_custom.cc```\n",
        "\n",
        "Next we'll deploy that model using the Arduino IDE."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "4-6-8-CustomDatasetKWSModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}